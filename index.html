<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Wake Vision Dataset</title>
    <style>
        :root {
            --primary-color: #4CAF50;
            --secondary-color: #8BC34A;
            --background-color: #E8F5E9;
            --text-color: #333;
            --header-bg: #F1F8E9;
        }
        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            background-color: var(--background-color);
            margin: 0;
            padding: 0;
        }
        .container {
            max-width: 1200px;
            margin: 0 auto;
            padding: 20px;
        }
        .contact {
            flex: 1;
            display: flex;
        }
        .logo-container {
            flex: 1;
            display: flex;
            justify-content: flex-end;
            min-width: 300px;
        }
        @media (max-width: 768px) {
            .contact {
                flex-direction: column;
                align-items: center;
            }
            .logo-container {
                width: 100%;
                justify-content: center;
                text-align: center;
                margin-bottom: 20px;
            }
        }
        header {
            background-color: var(--header-bg);
            box-shadow: 0 2px 10px rgba(0,0,0,0.1);
            z-index: 1000;
        }
        .header-content {
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
        }
        .title-container {
            flex: 1;
            min-width: 300px;
        }
        .logo-container {
            flex: 1;
            display: flex;
            justify-content: flex-end;
            min-width: 300px;
        }
        .logo {
            max-height: 160px;
            width: auto;
        }
        h1, h2 {
            color: var(--primary-color);
            margin: 0 0 15px 0;
        }
        h1 {
            font-size: 2.5em;
        }
        .button {
            display: inline-block;
            padding: 12px 24px;
            background-color: var(--primary-color);
            color: white;
            text-decoration: none;
            border-radius: 30px;
            margin-right: 10px;
            margin-bottom: 10px;
            transition: all 0.3s ease;
            box-shadow: 0 2px 5px rgba(0,0,0,0.1);
        }
        .button:hover {
            background-color: var(--secondary-color);
            transform: translateY(-2px);
            box-shadow: 0 4px 10px rgba(0,0,0,0.15);
        }
        .feature-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(200px, 1fr));
            gap: 20px;
            margin-top: 20px;
        }
        .feature-item {
            background-color: white;
            padding: 20px;
            border-radius: 10px;
            box-shadow: 0 2px 10px rgba(0,0,0,0.1);
            transition: all 0.3s ease;
        }
        .feature-item:hover {
            transform: translateY(-5px);
            box-shadow: 0 4px 15px rgba(0,0,0,0.15);
        }
        .sections-container {
            display: flex;
            flex-wrap: wrap;
            gap: 30px;
            margin-bottom: 30px;
        }
        .section {
            flex: 1;
            min-width: 300px;
            background-color: white;
            padding: 30px;
            border-radius: 10px;
            box-shadow: 0 2px 10px rgba(0,0,0,0.1);
        }
        footer {
            background-color: var(--primary-color);
            color: white;
            text-align: center;
            padding: 20px 0;
            margin-top: 40px;
        }
        @keyframes fadeIn {
            from { opacity: 0; transform: translateY(20px); }
            to { opacity: 1; transform: translateY(0); }
        }
        .fade-in {
            animation: fadeIn 0.5s ease-out forwards;
            opacity: 0;
        }
        .fade-in:nth-child(2) { animation-delay: 0.2s; }
        .fade-in:nth-child(3) { animation-delay: 0.4s; }
        .fade-in:nth-child(4) { animation-delay: 0.6s; }
        .anchor {
            display: block;
            position: relative;
            top: -10px;
            visibility: hidden;
        }
        @media (min-width: 1150px) {
            header {
                position: sticky;
                top: 0;
            }
            .anchor{
                top: -200px;
            }
        }
        @media (max-width: 768px) {
            .header-content {
                flex-direction: column;
                align-items: center;
            }
            .title-container, .logo-container {
                width: 100%;
                justify-content: center;
                text-align: center;
                margin-bottom: 20px;
            }
        }
        .image-grid {
        display: grid;
        grid-template-columns: repeat(auto-fit, minmax(250px, 1fr));
        gap: 20px;
        margin-top: 20px;
        }
        .image-item {
            background-color: white;
            border-radius: 10px;
            box-shadow: 0 2px 10px rgba(0,0,0,0.1);
            transition: all 0.3s ease;
            overflow: hidden;
            aspect-ratio: 1 / 1;
        }
        .image-item img {
            width: 100%;
            height: 100%;
            object-fit: cover;
            display: block;
        }
        .image-item:hover {
            transform: translateY(-5px);
            box-shadow: 0 4px 15px rgba(0,0,0,0.15);
        }
    </style>
</head>
<body>
    <header>
        <div class="container">
            <div class="header-content">
                <div class="title-container">
                    <h1>Wake Vision Dataset</h1>
                    <div class="fade-in">
                        <a href="https://github.com/colbybanbury/Wake_Vision_Quickstart" class="button">Quick Start Guide</a>
                        <a href="https://arxiv.org/abs/2405.00892" class="button">Read the Paper</a>
                        <a href="#access" class="button">Access the Dataset</a>
                    </div>
                </div>
                <div class="logo-container">
                    <img src="wake_vision_logo.png" alt="Wake Vision Logo" class="logo">
                </div>
            </div>
        </div>
    </header>
    
    <main class="container">
        <h2 class="fade-in">About</h2>
        <p class="fade-in">Wake Vision is a state-of-the-art person detection dataset specifically created for TinyML applications.
            It provides a comprehensive collection of high-quality images and precise annotations to train and evaluate machine learning models for efficient person detection on embedded and edge devices.
            Wake Vision also includes a fine-grain benchmark suite for evaluating the robustness of TinyML models.  </p>
        

        <div class="sections-container">
            <div class="section fade-in">
                <h2>The Dataset</h2>
                <p>Wake Vision is a large, high-quality binary image classifcation dataset for person detection:</p>
                <ul>
                    <li>Over 6 million high-quality images</li>
                    <li>Two training sets (Large & Quality)</li>
                    <li>High quality validation and test sets (~2% Label Error Rate)</li>
                </ul>
            </div>

            <div class="section fade-in">
                <h2>Fine-Grain Benchmark Suite</h2>
                <p>Wake Vision also incorporates a comprehensive fine-grained benchmark to assess fairness and robustness across:</p>
                <ul>
                    <li>Perceived gender</li>
                    <li>Perceived age</li>
                    <li>Subject distance</li>
                    <li>Lighting conditions</li>
                    <li>Depictions (e.g., drawings, digital renderings)</li>
                </ul>
            </div>
        </div>
        
        <a class="anchor" id="access"></a>
        <h2 class="fade-in">Access The Dataset</h2>
        <div class="fade-in">
            <a href="https://huggingface.co/datasets/Harvard-Edge/Wake-Vision" class="button">HuggingFace</a>
            <a href="https://www.tensorflow.org/datasets/catalog/wake_vision" class="button">TensorFlow Datasets</a>
            <a href="https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/1HOPXC" class="button">Download Directly</a>
        </div>

        <h2 class="fade-in">Key Features</h2>
        <div class="feature-grid">
            <div class="feature-item fade-in">
                <h3>TinyML Focus</h3>
                <p>TinyML relevant usescase and tractable task.</p>
            </div>
            <div class="feature-item fade-in">
                <h3>Two Training Sets</h3>
                <p>One large and one high quality, ideal for data-centric AI research</p>
            </div>
            <div class="feature-item fade-in">
                <h3>Diverse Scenarios</h3>
                <p>Wide range of person detection use cases</p>
            </div>
            <div class="feature-item fade-in">
                <h3>High-Quality Test and Val</h3>
                <p>Manually labeled to ensure reliable evaluation</p>
            </div>
        </div>
                <h2></h2>
        <h2 class="fade-in">Leaderboard</h2>
        <p class="fade-in">
            <table width="80%" style="margin: 0 auto; border:0px solid;text-align:center">
                <thead>
                    <tr>
                        <th align="center">Model Name (.tflite)</th>
                        <th align="center">Input Size</th>
                        <th align="center">RAM (KiB)</th>
                        <th align="center">Flash (KiB)</th>
                        <th align="center">MACs (MM)</th>
                        <th align="center">Test Accuracy</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td align="center"><a href="https://github.com/mit-han-lab/mcunet/blob/master/mcunet/model_zoo.py">mcunet-320kb-1mb_vww</a></td>
                        <td align="center">(144,144,3)</td>
                        <td align="center">393</td>
                        <td align="center">923.76</td>
                        <td align="center">56,022,934</td>
                        <td align="center">85.6±0.34%</td>
                    </tr>
                    <tr>
                        <td align="center"><a href="https://keras.io/api/applications/mobilenet/">MobileNetV2_0.25</a></td>
                        <td align="center">(224,224,3)</td>
                        <td align="center">1,244.5</td>
                        <td align="center">410.55</td>
                        <td align="center">36,453,732</td>
                        <td align="center">84.9±0.11%</td>
                    </tr>
                    <tr>
                        <td align="center"><a href="https://github.com/mit-han-lab/mcunet/blob/master/mcunet/model_zoo.py">mcunet-5fps_vww</a></td>
                        <td align="center">(80,80,3)</td>
                        <td align="center">226.5</td>
                        <td align="center">624.55</td>
                        <td align="center">11,645,502</td>
                        <td align="center">82.9±0.29%</td>
                    </tr>
                    <tr>
                        <td align="center"><a href="https://github.com/mit-han-lab/mcunet/blob/master/mcunet/model_zoo.py">mcunet-10fps_vww</a></td>
                        <td align="center">(64,64,3)</td>
                        <td align="center">168.5</td>
                        <td align="center">533.84</td>
                        <td align="center">5,998,334</td>
                        <td align="center">81.7±0.28%</td>
                    </tr>
                    <tr>
                        <td align="center"><a href="https://github.com/ARM-software/ML-zoo/tree/master/models/visual_wake_words">micronet_vww4_128_128_INT8</a></td>
                        <td align="center">(128,128,1)</td>
                        <td align="center">123.50</td>
                        <td align="center">417.03</td>
                        <td align="center">18,963,302</td>
                        <td align="center">77.9±0.6%</td>
                    </tr>
                    <tr>
                        <td align="center"><a href="https://github.com/ARM-software/ML-zoo/tree/master/models/visual_wake_words">micronet_vww3_128_128_INT8</a></td>
                        <td align="center">(128,128,1)</td>
                        <td align="center">137.50</td>
                        <td align="center">463.73</td>
                        <td align="center">22,690,291</td>
                        <td align="center">77.8±0.56%</td>
                    </tr>
                    <tr>
                        <td align="center"><a href="https://github.com/AndreaMattiaGaravagno/ColabNAS/tree/main/models">colabnask_8_c_5</a></td>
                        <td align="center">(50,50,3)</td>
                        <td align="center">32.5</td>
                        <td align="center">44.56</td>
                        <td align="center">2,135,476</td>
                        <td align="center">77.3±0.37%</td>
                    </tr>
                    <tr>
                        <td align="center"><a href="https://github.com/AndreaMattiaGaravagno/ColabNAS/tree/main/models">colabnask_4_c_5</a></td>
                        <td align="center">(50,50,3)</td>
                        <td align="center">22</td>
                        <td align="center">18.49</td>
                        <td align="center">688,790</td>
                        <td align="center">75.7±0.18%</td>
                    </tr>
                    <tr>
                        <td align="center"><a href="https://github.com/ARM-software/ML-zoo/tree/master/models/visual_wake_words">micronet_vww2_50_50_INT8</a></td>
                        <td align="center">(50,50,1)</td>
                        <td align="center">71.50</td>
                        <td align="center">225.54</td>
                        <td align="center">3,167,382</td>
                        <td align="center">71.9±0.67%</td>
                    </tr>
                    <tr>
                        <td align="center"><a href="https://github.com/AndreaMattiaGaravagno/ColabNAS/tree/main/models">colabnas_k_2_c_3</a></td>
                        <td align="center">(50,50,3)</td>
                        <td align="center">18.5</td>
                        <td align="center">7.66</td>
                        <td align="center">250,256</td>
                        <td align="center">70.6±0.96%</td>
                    </tr>
                </tbody>
            </table>
        </p>
        <h2></h2>
        <h2 class="fade-in">Example Images</h2>
        <div class="image-grid">
            <div class="image-item fade-in">
                <img src="female_person.png" alt="Predominantly Female Person">
            </div>
            <div class="image-item fade-in">
                <img src="bright_image.png" alt="Bright Image">
            </div>
            <div class="image-item fade-in">
                <img src="depiction_person.png" alt="Depicted Person">
            </div>
            <div class="image-item fade-in">
                <img src="young_person.png" alt="Young Person">
            </div>
        </div>
        <h2></h2>
        <h2 class="fade-in">License</h2>
        <p class="fade-in">The Wake Vision labels are derived from Open Image's annotations which are licensed by Google LLC under CC BY 4.0 license. The images are listed as having a CC BY 2.0 license. Note from Open Images: "while we tried to identify images that are licensed under a Creative Commons Attribution license, we make no representations or warranties regarding the license status of each image and you should verify the license for each image yourself."</p>

        <h2></h2>
        <h2>Cite</h2>
        <section id="cite" class="section">
            @article{banbury2024wake,<br>
title={Wake Vision: A Large-scale, Diverse Dataset and Benchmark Suite for TinyML Person Detection},<br>
author={Banbury, Colby and Njor, Emil
and Stewart, Matthew and Warden, Pete
and Kudlur, Manjunath and Jeffries, Nat
and Fafoutis, Xenofon and Reddi, Vijay Janapa},<br>
journal={arXiv preprint arXiv:2405.00892},<br>
year={2024}<br>
}
          </section>
          <h2></h2>
          <div class="contact">
            <div>
                <h2 class="fade-in">Contact</h2>
                <p class="fade-in">Email: <a href="mailto:emjn@dtu.dk">emjn@dtu.dk</a>
                    <a href="mailto:cbanbury@g.harvard.edu">cbanbury@g.harvard.edu</a></p>
            </div>
            <div class="logo-container">
                <img src="Harvard_logo.png" alt="Harvard SEAS Logo" class="logo">
            </div>
        </div>

    </main>
    
    <footer>
        <div class="container">
            <p>&copy; 2024 Wake Vision Dataset. All rights reserved.</p>
        </div>
    </footer>
</body>
</html>
