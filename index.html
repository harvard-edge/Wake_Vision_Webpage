<!DOCTYPE html>
<html lang="en">
   <head>
      <meta charset="UTF-8">
      <meta name="viewport" content="width=device-width, initial-scale=1.0">
      <title>Wake Vision Dataset</title>
      <link rel="stylesheet" href="styles.css">
   </head>
   <script>
      document.addEventListener("DOMContentLoaded", () => {
         const table = document.querySelector(".leaderboard-container table");
         const headers = table.querySelectorAll("thead th");
         const tbody = table.querySelector("tbody");
      
         const defaultSortColumnIndex = 5; // Index of the "Test Accuracy" column
      
         headers.forEach((header, index) => {
            // Create arrow span once for each header
            const arrow = document.createElement("span");
            arrow.classList.add("arrow");
            header.appendChild(arrow);
      
            header.addEventListener("click", () => {
               const isAscending = header.classList.toggle("sorted-asc");
               header.classList.toggle("sorted-desc", !isAscending);
      
               // Update arrow directions for all headers
               headers.forEach((h, i) => {
                  const hArrow = h.querySelector(".arrow");
                  if (i !== index) {
                     h.classList.remove("sorted-asc", "sorted-desc");
                     hArrow.textContent = ""; // Clear arrow for non-sorted columns
                  } else {
                     hArrow.textContent = isAscending ? "▲" : "▼"; // Set arrow direction
                  }
               });
      
               const rows = Array.from(tbody.querySelectorAll("tr"));
               const compare = (rowA, rowB) => {
                  const cellA = rowA.children[index].textContent.trim();
                  const cellB = rowB.children[index].textContent.trim();
      
                  const valueA = parseCellValue(cellA);
                  const valueB = parseCellValue(cellB);
      
                  if (valueA < valueB) return isAscending ? -1 : 1;
                  if (valueA > valueB) return isAscending ? 1 : -1;
                  return 0;
               };
      
               rows.sort(compare);
               rows.forEach(row => tbody.appendChild(row)); // Re-append sorted rows
            });
         });
      
         // Helper function to parse cell values
         const parseCellValue = (value) => {
            // Handle percentages
            if (value.includes('%')) {
               return parseFloat(value.replace(/[^\d.-]/g, ''));
            }
      
            // Handle numbers with commas
            if (value.includes(',')) {
               return parseFloat(value.replace(/,/g, ''));
            }
      
            // Default to text if not numeric
            return isNaN(value) ? value.toLowerCase() : parseFloat(value);
         };
      
         // Trigger default sorting by "Test Accuracy" in descending order
         const defaultSortHeader = headers[defaultSortColumnIndex];
         defaultSortHeader.classList.add("sorted-desc"); // Add descending class
         const rows = Array.from(tbody.querySelectorAll("tr"));
         const compare = (rowA, rowB) => {
            const cellA = rowA.children[defaultSortColumnIndex].textContent.trim();
            const cellB = rowB.children[defaultSortColumnIndex].textContent.trim();
      
            const valueA = parseCellValue(cellA);
            const valueB = parseCellValue(cellB);
      
            if (valueA < valueB) return 1; // Reverse logic for descending
            if (valueA > valueB) return -1;
            return 0;
         };
      
         rows.sort(compare);
         rows.forEach(row => tbody.appendChild(row)); // Re-append sorted rows
         defaultSortHeader.querySelector(".arrow").textContent = "▼"; // Set descending arrow
      });
   </script>
   <body>
      <header>
         <div class="container">
            <div class="header-content">
               <div class="title-container">
                  <h1>Wake Vision Dataset</h1>
                  <div class="fade-in">
                     <a href="https://github.com/colbybanbury/Wake_Vision_Quickstart" class="button">Quick Start Guide</a>
                     <a href="https://arxiv.org/abs/2405.00892" class="button">Read the Paper</a>
                     <a href="#access" class="button">Access the Dataset</a>
                  </div>
               </div>
               <div class="logo-container">
                  <img src="wake_vision_logo.png" alt="Wake Vision Logo" class="logo">
               </div>
            </div>
         </div>
      </header>
      <main class="container">
         <h2 class="fade-in">About</h2>
         <p class="fade-in">Wake Vision is a state-of-the-art person detection dataset specifically created for TinyML applications.
            It provides a comprehensive collection of high-quality images and precise annotations to train and evaluate machine learning models for efficient person detection on embedded and edge devices.
            Wake Vision also includes a fine-grain benchmark suite for evaluating the robustness of TinyML models.  
         </p>
         <div class="sections-container">
            <div class="section fade-in">
               <h2>The Dataset</h2>
               <p>Wake Vision is a large, high-quality binary image classifcation dataset for person detection:</p>
               <ul>
                  <li>Over 6 million high-quality images</li>
                  <li>Two training sets (Large & Quality)</li>
                  <li>High quality validation and test sets (~2% Label Error Rate)</li>
               </ul>
            </div>
            <div class="section fade-in">
               <h2>Fine-Grain Benchmark Suite</h2>
               <p>Wake Vision also incorporates a comprehensive fine-grained benchmark to assess fairness and robustness across:</p>
               <ul>
                  <li>Perceived gender</li>
                  <li>Perceived age</li>
                  <li>Subject distance</li>
                  <li>Lighting conditions</li>
                  <li>Depictions (e.g., drawings, digital renderings)</li>
               </ul>
            </div>
         </div>
         <a class="anchor" id="access"></a>
         <h2 class="fade-in">Access The Dataset</h2>
         <div class="fade-in">
            <a href="https://huggingface.co/datasets/Harvard-Edge/Wake-Vision" class="button">HuggingFace</a>
            <a href="https://www.tensorflow.org/datasets/catalog/wake_vision" class="button">TensorFlow Datasets</a>
            <a href="https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/1HOPXC" class="button">Download Directly</a>
         </div>
         <h2 class="fade-in">Key Features</h2>
         <div class="feature-grid">
            <div class="feature-item fade-in">
               <h3>TinyML Focus</h3>
               <p>TinyML relevant usescase and tractable task.</p>
            </div>
            <div class="feature-item fade-in">
               <h3>Two Training Sets</h3>
               <p>One large and one high quality, ideal for data-centric AI research</p>
            </div>
            <div class="feature-item fade-in">
               <h3>Diverse Scenarios</h3>
               <p>Wide range of person detection use cases</p>
            </div>
            <div class="feature-item fade-in">
               <h3>High-Quality Test and Val</h3>
               <p>Manually labeled to ensure reliable evaluation</p>
            </div>
         </div>
         <h2></h2>
         <a class="anchor" id="leaderboard"></a>
         <h2 class="fade-in">Leaderboard</h2>
         <div class="leaderboard-wrapper fade-in">
            <div class="leaderboard-container">
               <table width="80%" style="margin: 0 auto; border:0px solid;text-align:center">
                  <thead>
                     <tr>
                        <th>Model Name</th>
                        <th>Input Size</th>
                        <th>RAM (KiB)</th>
                        <th>Flash (KiB)</th>
                        <th>MACs</th>
                        <th>Test Accuracy</th>
                     </tr>
                  </thead>
                  <tbody>
                     <tr>
                        <td align="center"><a href="https://github.com/harvard-edge/Wake_Vision/blob/main/experiments/comprehensive_model_architecture_experiments/wake_vision_quality/mcunet-320kb-1mb_vww.py">mcunet-vww2</a></td>
                        <td align="center">(144,144,3)</td>
                        <td align="center">393</td>
                        <td align="center">923.76</td>
                        <td align="center">56,022,934</td>
                        <td align="center">85.6±0.34%</td>
                     </tr>
                     <tr>
                        <td align="center"><a href="https://keras.io/api/applications/mobilenet/">MobileNetV2_0.25</a></td>
                        <td align="center">(224,224,3)</td>
                        <td align="center">1,244.5</td>
                        <td align="center">410.55</td>
                        <td align="center">36,453,732</td>
                        <td align="center">84.9±0.11%</td>
                     </tr>
                     <tr>
                        <td align="center"><a href="https://github.com/harvard-edge/Wake_Vision/blob/main/experiments/comprehensive_model_architecture_experiments/wake_vision_quality/mcunet-5fps_vww.py">mcunet-vww1</a></td>
                        <td align="center">(80,80,3)</td>
                        <td align="center">226.5</td>
                        <td align="center">624.55</td>
                        <td align="center">11,645,502</td>
                        <td align="center">82.9±0.29%</td>
                     </tr>
                     <tr>
                        <td align="center"><a href="https://github.com/harvard-edge/Wake_Vision/blob/main/experiments/comprehensive_model_architecture_experiments/wake_vision_quality/mcunet-10fps_vww.py">mcunet-vww0</a></td>
                        <td align="center">(64,64,3)</td>
                        <td align="center">168.5</td>
                        <td align="center">533.84</td>
                        <td align="center">5,998,334</td>
                        <td align="center">81.7±0.28%</td>
                     </tr>
                     <tr>
                        <td align="center"><a href="https://github.com/harvard-edge/Wake_Vision/blob/main/experiments/comprehensive_model_architecture_experiments/wake_vision_quality/micronets_vww4_128_128_INT8.py">micronet_vww4</a></td>
                        <td align="center">(128,128,1)</td>
                        <td align="center">123.50</td>
                        <td align="center">417.03</td>
                        <td align="center">18,963,302</td>
                        <td align="center">77.9±0.6%</td>
                     </tr>
                     <tr>
                        <td align="center"><a href="https://github.com/harvard-edge/Wake_Vision/blob/main/experiments/comprehensive_model_architecture_experiments/wake_vision_quality/micronets_vww3_128_128_INT8.py">micronet_vww3</a></td>
                        <td align="center">(128,128,1)</td>
                        <td align="center">137.50</td>
                        <td align="center">463.73</td>
                        <td align="center">22,690,291</td>
                        <td align="center">77.8±0.56%</td>
                     </tr>
                     <tr>
                        <td align="center"><a href="https://github.com/harvard-edge/Wake_Vision/blob/main/experiments/comprehensive_model_architecture_experiments/wake_vision_quality/k_8_c_5.py">colabnas_k_8</a></td>
                        <td align="center">(50,50,3)</td>
                        <td align="center">32.5</td>
                        <td align="center">44.56</td>
                        <td align="center">2,135,476</td>
                        <td align="center">77.3±0.37%</td>
                     </tr>
                     <tr>
                        <td align="center"><a href="https://github.com/harvard-edge/Wake_Vision/blob/main/experiments/comprehensive_model_architecture_experiments/wake_vision_quality/k_4_c_5.py">colabnas_k_4</a></td>
                        <td align="center">(50,50,3)</td>
                        <td align="center">22</td>
                        <td align="center">18.49</td>
                        <td align="center">688,790</td>
                        <td align="center">75.7±0.18%</td>
                     </tr>
                     <tr>
                        <td align="center"><a href="https://github.com/harvard-edge/Wake_Vision/blob/main/experiments/comprehensive_model_architecture_experiments/wake_vision_quality/micronets_vww2_50_50_INT8.py">micronet_vww2</a></td>
                        <td align="center">(50,50,1)</td>
                        <td align="center">71.50</td>
                        <td align="center">225.54</td>
                        <td align="center">3,167,382</td>
                        <td align="center">71.9±0.67%</td>
                     </tr>
                     <tr>
                        <td align="center"><a href="https://github.com/harvard-edge/Wake_Vision/blob/main/experiments/comprehensive_model_architecture_experiments/wake_vision_quality/k_2_c_3.py">colabnas_k_2</a></td>
                        <td align="center">(50,50,3)</td>
                        <td align="center">18.5</td>
                        <td align="center">7.66</td>
                        <td align="center">250,256</td>
                        <td align="center">70.6±0.96%</td>
                     </tr>
                  </tbody>
               </table>
            </div>
         </div>
         </p>
         <div class="feature-grid">
            <div class="feature-grid">
               <div class="feature-item fade-in">
                  <h3>🙋‍♂️ Contribute</h3>
                  <p>
                     <a href="mailto:AndreaMattia.Garavagno@santannapisa.it">Share your results with us</a> 
                     and contribute to the leaderboard, or you can issue a PR at 
                     <a href="https://github.com/harvard-edge/Wake_Vision_Webpage/pulls">this link</a>!
                  </p>
               </div>
               <div class="feature-item fade-in">
                  <h3>🏆 Challenge</h3>
                  <p>The first edition of the <a href="https://edgeai.modelnova.ai/challenges/details/1">Wake Vision Challenge</a> is online!</p>
               </div>
            </div>
         </div>
         <h2></h2>
         <h2 class="fade-in">Example Images</h2>
         <div class="image-grid">
            <div class="image-item fade-in">
               <img src="female_person.png" alt="Predominantly Female Person">
            </div>
            <div class="image-item fade-in">
               <img src="bright_image.png" alt="Bright Image">
            </div>
            <div class="image-item fade-in">
               <img src="depiction_person.png" alt="Depicted Person">
            </div>
            <div class="image-item fade-in">
               <img src="young_person.png" alt="Young Person">
            </div>
         </div>
         <h2></h2>
         <h2 class="fade-in">License</h2>
         <p class="fade-in">The Wake Vision labels are derived from Open Image's annotations which are licensed by Google LLC under CC BY 4.0 license. The images are listed as having a CC BY 2.0 license. Note from Open Images: "while we tried to identify images that are licensed under a Creative Commons Attribution license, we make no representations or warranties regarding the license status of each image and you should verify the license for each image yourself."</p>
         <h2></h2>
         <h2>Cite</h2>
         <section id="cite" class="section">
            @article{banbury2024wake,<br>
            title={Wake Vision: A Tailored Dataset and Benchmark Suite for TinyML Computer Vision Applications},<br>
            author={Banbury, Colby and Njor, Emil
            and Garavagno, Andrea Mattia and 
            Stewart, Matthew and Warden, Pete
            and Kudlur, Manjunath and Jeffries, Nat
            and Fafoutis, Xenofon and Reddi, Vijay Janapa},<br>
            journal={arXiv preprint arXiv:2405.00892},<br>
            year={2024}<br>
            }
         </section>
         <h2></h2>
         <div class="contact">
            <div>
               <h2 class="fade-in">Contact</h2>
               <p class="fade-in">Email: <a href="mailto:emjn@dtu.dk">emjn@dtu.dk</a>
                  <a href="mailto:cbanbury@g.harvard.edu">cbanbury@g.harvard.edu</a>
                  <a href="mailto:AndreaMattia.Garavagno@edu.unige.it">AndreaMattia.Garavagno@edu.unige.it</a>
               </p>
            </div>
            <div class="logo-container">
               <img src="Harvard_logo.png" alt="Harvard SEAS Logo" class="logo">
            </div>
         </div>
      </main>
      <footer>
         <div class="container">
            <p>&copy; 2024 Wake Vision Dataset. All rights reserved.</p>
         </div>
      </footer>
   </body>
</html>
